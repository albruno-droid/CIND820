{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5452ee2a"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the 'BeijingPM20100101_20151231.csv' file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3203fc0"
      },
      "source": [
        "First, I'll import the pandas library to enable data manipulation. Then, I'll load the specified CSV file, 'BeijingPM20100101_20151231.csv', into a pandas DataFrame named 'df'. This action directly addresses the subtask's requirement to load the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70e068ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48470b48-a2ae-4454-b581-d494ddd3f226"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/BeijingPM20100101_20151231.csv')\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   No  year  month  day  hour  season  PM_Dongsi  PM_Dongsihuan  \\\n",
            "0   1  2010      1    1     0       4        NaN            NaN   \n",
            "1   2  2010      1    1     1       4        NaN            NaN   \n",
            "2   3  2010      1    1     2       4        NaN            NaN   \n",
            "3   4  2010      1    1     3       4        NaN            NaN   \n",
            "4   5  2010      1    1     4       4        NaN            NaN   \n",
            "\n",
            "   PM_Nongzhanguan  PM_US Post  DEWP  HUMI    PRES  TEMP cbwd    Iws  \\\n",
            "0              NaN         NaN -21.0  43.0  1021.0 -11.0   NW   1.79   \n",
            "1              NaN         NaN -21.0  47.0  1020.0 -12.0   NW   4.92   \n",
            "2              NaN         NaN -21.0  43.0  1019.0 -11.0   NW   6.71   \n",
            "3              NaN         NaN -21.0  55.0  1019.0 -14.0   NW   9.84   \n",
            "4              NaN         NaN -20.0  51.0  1018.0 -12.0   NW  12.97   \n",
            "\n",
            "   precipitation  Iprec  \n",
            "0            0.0    0.0  \n",
            "1            0.0    0.0  \n",
            "2            0.0    0.0  \n",
            "3            0.0    0.0  \n",
            "4            0.0    0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec312904"
      },
      "source": [
        "## Inspect Data\n",
        "Display the first few rows of the DataFrame, check its information (data types, non-null counts), and describe basic statistics to understand its structure and content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d198fd4"
      },
      "source": [
        "To understand the DataFrame's structure and content, I will display its information, including data types and non-null counts, using `df.info()` and then generate descriptive statistics for numerical columns using `df.describe()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88c21cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b9c0ab-c365-429e-a1b3-10f2e3db0545"
      },
      "source": [
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 52584 entries, 0 to 52583\n",
            "Data columns (total 18 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   No               52584 non-null  int64  \n",
            " 1   year             52584 non-null  int64  \n",
            " 2   month            52584 non-null  int64  \n",
            " 3   day              52584 non-null  int64  \n",
            " 4   hour             52584 non-null  int64  \n",
            " 5   season           52584 non-null  int64  \n",
            " 6   PM_Dongsi        25052 non-null  float64\n",
            " 7   PM_Dongsihuan    20508 non-null  float64\n",
            " 8   PM_Nongzhanguan  24931 non-null  float64\n",
            " 9   PM_US Post       50387 non-null  float64\n",
            " 10  DEWP             52579 non-null  float64\n",
            " 11  HUMI             52245 non-null  float64\n",
            " 12  PRES             52245 non-null  float64\n",
            " 13  TEMP             52579 non-null  float64\n",
            " 14  cbwd             52579 non-null  object \n",
            " 15  Iws              52579 non-null  float64\n",
            " 16  precipitation    52100 non-null  float64\n",
            " 17  Iprec            52100 non-null  float64\n",
            "dtypes: float64(11), int64(6), object(1)\n",
            "memory usage: 7.2+ MB\n",
            "None\n",
            "                 No          year         month           day          hour  \\\n",
            "count  52584.000000  52584.000000  52584.000000  52584.000000  52584.000000   \n",
            "mean   26292.500000   2012.499772      6.523962     15.726609     11.500000   \n",
            "std    15179.837614      1.707485      3.448452      8.798896      6.922252   \n",
            "min        1.000000   2010.000000      1.000000      1.000000      0.000000   \n",
            "25%    13146.750000   2011.000000      4.000000      8.000000      5.750000   \n",
            "50%    26292.500000   2012.000000      7.000000     16.000000     11.500000   \n",
            "75%    39438.250000   2014.000000     10.000000     23.000000     17.250000   \n",
            "max    52584.000000   2015.000000     12.000000     31.000000     23.000000   \n",
            "\n",
            "             season     PM_Dongsi  PM_Dongsihuan  PM_Nongzhanguan  \\\n",
            "count  52584.000000  25052.000000   20508.000000     24931.000000   \n",
            "mean       2.491100     89.154439      92.560806        88.643737   \n",
            "std        1.116988     87.239267      88.027434        88.041166   \n",
            "min        1.000000      3.000000       3.000000         3.000000   \n",
            "25%        1.000000     24.000000      28.000000        24.000000   \n",
            "50%        2.000000     64.000000      68.000000        62.000000   \n",
            "75%        3.000000    124.000000     127.000000       122.000000   \n",
            "max        4.000000    737.000000     672.000000       844.000000   \n",
            "\n",
            "         PM_US Post          DEWP          HUMI          PRES          TEMP  \\\n",
            "count  50387.000000  52579.000000  52245.000000  52245.000000  52579.000000   \n",
            "mean      95.904241      2.074554     54.602421   1016.465442     12.587040   \n",
            "std       91.643772     14.222059     25.991338     10.295070     12.098527   \n",
            "min        1.000000    -40.000000      2.000000    991.000000    -19.000000   \n",
            "25%       27.000000    -10.000000     31.000000   1008.000000      2.000000   \n",
            "50%       69.000000      2.000000     55.000000   1016.000000     14.000000   \n",
            "75%      132.000000     15.000000     78.000000   1025.000000     23.000000   \n",
            "max      994.000000     28.000000    100.000000   1046.000000     42.000000   \n",
            "\n",
            "                Iws  precipitation          Iprec  \n",
            "count  52579.000000   52100.000000   52100.000000  \n",
            "mean      23.261829      19.258683      19.519008  \n",
            "std       49.281706    4381.035532    4381.036040  \n",
            "min        0.450000       0.000000       0.000000  \n",
            "25%        1.790000       0.000000       0.000000  \n",
            "50%        4.920000       0.000000       0.000000  \n",
            "75%       21.020000       0.000000       0.000000  \n",
            "max      585.600000  999990.000000  999990.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a09ea91a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset, named \"BeijingPM20100101\\_20151231.csv\", contains 52,584 entries and 18 columns.\n",
        "*   The columns include temporal information (`year`, `month`, `day`, `hour`), air quality measurements (e.g., `PM_Dongsi`, `PM_US Post`), and meteorological features (e.g., `DEWP`, `HUMI`, `PRES`, `TEMP`, `Iws`, `precipitation`, `Iprec`, `cbwd`).\n",
        "*   The DataFrame consists of 11 float64 columns, 6 int64 columns, and 1 object column (`cbwd`).\n",
        "*   Significant missing values are present, particularly in the PM-related columns (`PM_Dongsi`, `PM_Dongsihuan`, `PM_Nongzhanguan`, `PM_US Post`). Other columns like `DEWP`, `HUMI`, `PRES`, `TEMP`, `cbwd`, `Iws`, `precipitation`, and `Iprec` also have a small number of missing entries.\n",
        "*   Descriptive statistics reveal a suspicious maximum value of 999,990.0 in both `precipitation` and `Iprec` columns, suggesting a potential data entry error or a special code.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Further investigation is required to understand and address the anomalous maximum values in the `precipitation` and `Iprec` columns, possibly indicating a sentinel value for no precipitation.\n",
        "*   A comprehensive strategy for handling missing values, especially in the PM-related columns, needs to be developed before conducting any in-depth analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2f1e64b"
      },
      "source": [
        "# Task\n",
        "Prepare the `df` DataFrame for a classification task by identifying a target variable and features, handling missing values in columns including 'PM_Dongsi', 'PM_Dongsihuan', 'PM_Nongzhanguan', 'PM_US Post', 'DEWP', 'HUMI', 'PRES', 'TEMP', 'cbwd', 'Iws', 'precipitation', and 'Iprec', addressing anomalous values like 999,990.0 in 'precipitation' and 'Iprec', and encoding the categorical feature 'cbwd'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8488f1b"
      },
      "source": [
        "## Identify Target and Features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "736cbdef"
      },
      "source": [
        "### Target Variable Selection:\n",
        "\n",
        "For this classification task, the target variable will be `PM_US Post`. This column represents PM2.5 concentration readings from the US Embassy in Beijing. It is chosen because it has the highest number of non-null observations among all PM-related columns, making it the most complete and potentially reliable for building a classification model. The goal will be to classify air quality based on this variable.\n",
        "\n",
        "### Feature Selection:\n",
        "\n",
        "The features for the model will include the temporal information (`year`, `month`, `day`, `hour`), other air quality measurements (`PM_Dongsi`, `PM_Dongsihuan`, `PM_Nongzhanguan`), and meteorological features (`DEWP`, `HUMI`, `PRES`, `TEMP`, `cbwd`, `Iws`, `precipitation`, `Iprec`). The 'No' column will be excluded as it is an identifier and does not contribute to the predictive power of the model. All selected features are considered relevant for predicting air quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c951b1ff"
      },
      "source": [
        "Now that the target variable and features have been identified, I will create separate DataFrames for the features (X) and the target variable (y) based on the selections made, excluding the 'No' column from features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ace9c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac03e47-b080-4d02-feea-5d4ed878286c"
      },
      "source": [
        "target_variable = 'PM_US Post'\n",
        "features = [col for col in df.columns if col not in ['No', target_variable]]\n",
        "\n",
        "X = df[features]\n",
        "y = df[target_variable]\n",
        "\n",
        "print(\"Shape of features (X):\", X.shape)\n",
        "print(\"Shape of target (y):\", y.shape)\n",
        "print(\"\\nFirst 5 rows of X:\")\n",
        "print(X.head())\n",
        "print(\"\\nFirst 5 rows of y:\")\n",
        "print(y.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of features (X): (52584, 16)\n",
            "Shape of target (y): (52584,)\n",
            "\n",
            "First 5 rows of X:\n",
            "   year  month  day  hour  season  PM_Dongsi  PM_Dongsihuan  PM_Nongzhanguan  \\\n",
            "0  2010      1    1     0       4        NaN            NaN              NaN   \n",
            "1  2010      1    1     1       4        NaN            NaN              NaN   \n",
            "2  2010      1    1     2       4        NaN            NaN              NaN   \n",
            "3  2010      1    1     3       4        NaN            NaN              NaN   \n",
            "4  2010      1    1     4       4        NaN            NaN              NaN   \n",
            "\n",
            "   DEWP  HUMI    PRES  TEMP cbwd    Iws  precipitation  Iprec  \n",
            "0 -21.0  43.0  1021.0 -11.0   NW   1.79            0.0    0.0  \n",
            "1 -21.0  47.0  1020.0 -12.0   NW   4.92            0.0    0.0  \n",
            "2 -21.0  43.0  1019.0 -11.0   NW   6.71            0.0    0.0  \n",
            "3 -21.0  55.0  1019.0 -14.0   NW   9.84            0.0    0.0  \n",
            "4 -20.0  51.0  1018.0 -12.0   NW  12.97            0.0    0.0  \n",
            "\n",
            "First 5 rows of y:\n",
            "0   NaN\n",
            "1   NaN\n",
            "2   NaN\n",
            "3   NaN\n",
            "4   NaN\n",
            "Name: PM_US Post, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d05b1c31"
      },
      "source": [
        "## Handle Missing Values\n",
        "\n",
        "Address the missing values in the dataset by dropping rows with missing target values, and then imputing numerical and categorical features in the feature set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63ac21a4"
      },
      "source": [
        "First, I will remove rows from both the feature DataFrame X and the target Series y where the target variable y has missing values to ensure data consistency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b4ba4d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437b6237-64a0-40a0-e56e-34e80b605fe3"
      },
      "source": [
        "initial_rows = X.shape[0]\n",
        "\n",
        "# Drop rows where the target variable 'y' has missing values\n",
        "drop_indices = y[y.isnull()].index\n",
        "X = X.drop(drop_indices)\n",
        "y = y.drop(drop_indices)\n",
        "\n",
        "print(f\"Dropped {initial_rows - X.shape[0]} rows with missing target values.\")\n",
        "print(\"Shape of X after dropping missing target values:\", X.shape)\n",
        "print(\"Shape of y after dropping missing target values:\", y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 2197 rows with missing target values.\n",
            "Shape of X after dropping missing target values: (50387, 16)\n",
            "Shape of y after dropping missing target values: (50387,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ffca140"
      },
      "source": [
        "Now that rows with missing target values have been removed, I will proceed to impute missing values in numerical columns with their respective medians and in the categorical column 'cbwd' with its mode, as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f970a67d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460c23cb-222c-4ac3-a1e3-e895046298eb"
      },
      "source": [
        "numerical_cols_to_impute = [\n",
        "    'PM_Dongsi', 'PM_Dongsihuan', 'PM_Nongzhanguan', 'DEWP', 'HUMI',\n",
        "    'PRES', 'TEMP', 'Iws', 'precipitation', 'Iprec'\n",
        "]\n",
        "\n",
        "for col in numerical_cols_to_impute:\n",
        "    if X[col].isnull().any():\n",
        "        median_val = X[col].median()\n",
        "        X.loc[:, col] = X[col].fillna(median_val) # Fixed: Assign directly to avoid SettingWithCopyWarning\n",
        "        print(f\"Imputed missing values in numerical column '{col}' with median: {median_val}\")\n",
        "\n",
        "categorical_col_to_impute = 'cbwd'\n",
        "if X[categorical_col_to_impute].isnull().any():\n",
        "    mode_val = X[categorical_col_to_impute].mode()[0] # .mode() can return multiple values, take the first\n",
        "    X.loc[:, categorical_col_to_impute] = X[categorical_col_to_impute].fillna(mode_val) # Fixed: Assign directly to avoid SettingWithCopyWarning\n",
        "    print(f\"Imputed missing values in categorical column '{categorical_col_to_impute}' with mode: {mode_val}\")\n",
        "\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(X.isnull().sum()[X.isnull().sum() > 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values after imputation:\n",
            "Series([], dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eda79450"
      },
      "source": [
        "## Handle Anomalous Values\n",
        "\n",
        "Investigate and address the anomalous maximum values (999,990.0) in the 'precipitation' and 'Iprec' columns in the feature set `X`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa928af3"
      },
      "source": [
        "To address the anomalous values, I will replace all instances of 999990.0 with 0.0 in both 'precipitation' and 'Iprec' columns, then print the maximum values to verify the replacement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98c31b8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d3117b-9c9d-4be2-ea9d-3826f826f9ab"
      },
      "source": [
        "X.loc[X['precipitation'] == 999990.0, 'precipitation'] = 0.0\n",
        "X.loc[X['Iprec'] == 999990.0, 'Iprec'] = 0.0\n",
        "\n",
        "print(\"Maximum value in 'precipitation' after addressing anomalies:\", X['precipitation'].max())\n",
        "print(\"Maximum value in 'Iprec' after addressing anomalies:\", X['Iprec'].max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum value in 'precipitation' after addressing anomalies: 69.2\n",
            "Maximum value in 'Iprec' after addressing anomalies: 223.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f8b607"
      },
      "source": [
        "## Encode Categorical Features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ffc302"
      },
      "source": [
        "To convert the categorical feature 'cbwd' into a numerical format, I will apply one-hot encoding using `pd.get_dummies()` and then display the updated DataFrame's shape and head to confirm the transformation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36fbbc83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3495dee-c8d8-45b3-e1a1-5485a51dbbfd"
      },
      "source": [
        "X = pd.get_dummies(X, columns=['cbwd'], drop_first=True)\n",
        "\n",
        "print(\"Shape of X after one-hot encoding:\", X.shape)\n",
        "print(\"\\nFirst 5 rows of X after one-hot encoding:\")\n",
        "print(X.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after one-hot encoding: (50387, 18)\n",
            "\n",
            "First 5 rows of X after one-hot encoding:\n",
            "    year  month  day  hour  season  PM_Dongsi  PM_Dongsihuan  PM_Nongzhanguan  \\\n",
            "23  2010      1    1    23       4       64.0           68.0             62.0   \n",
            "24  2010      1    2     0       4       64.0           68.0             62.0   \n",
            "25  2010      1    2     1       4       64.0           68.0             62.0   \n",
            "26  2010      1    2     2       4       64.0           68.0             62.0   \n",
            "27  2010      1    2     3       4       64.0           68.0             62.0   \n",
            "\n",
            "    DEWP  HUMI    PRES  TEMP   Iws  precipitation  Iprec  cbwd_NW  cbwd_SE  \\\n",
            "23 -17.0  41.0  1020.0  -5.0  0.89            0.0    0.0    False    False   \n",
            "24 -16.0  38.0  1020.0  -4.0  1.79            0.0    0.0    False     True   \n",
            "25 -15.0  42.0  1020.0  -4.0  2.68            0.0    0.0    False     True   \n",
            "26 -11.0  63.5  1021.0  -5.0  3.57            0.0    0.0    False     True   \n",
            "27  -7.0  85.0  1022.0  -5.0  5.36            0.0    0.0    False     True   \n",
            "\n",
            "    cbwd_cv  \n",
            "23     True  \n",
            "24    False  \n",
            "25    False  \n",
            "26    False  \n",
            "27    False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27ae5c18"
      },
      "source": [
        "Summarize the data preparation steps taken, including how missing values and anomalous values were handled, and how categorical features were encoded, along with the final state of the dataset ready for classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c876ead"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The data preparation steps involved:\n",
        "1.  **Missing Values Handling**: Rows with missing `PM_US Post` (target variable) values were dropped. Missing numerical values in columns such as 'PM_Dongsi', 'PM_Dongsihuan', 'PM_Nongzhanguan', 'DEWP', 'HUMI', 'PRES', 'TEMP', 'Iws', 'precipitation', and 'Iprec' were imputed using the median. Missing values in the categorical 'cbwd' column were imputed using the mode.\n",
        "2.  **Anomalous Values Handling**: Anomalous values of 999,990.0 in 'precipitation' and 'Iprec' were replaced with 0.0.\n",
        "3.  **Categorical Feature Encoding**: The 'cbwd' categorical feature was one-hot encoded using `pd.get_dummies()` with `drop_first=True`.\n",
        "4.  **Final State of the Dataset**: The final dataset `X` has a shape of (50387, 18) and `y` has a shape of (50387,). All specified missing and anomalous values have been addressed, and the categorical feature 'cbwd' is now numerically encoded, making the dataset ready for classification tasks.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `PM_US Post` column was selected as the target variable for classification.\n",
        "*   `2197` rows containing missing values in the target variable `PM_US Post` were removed, reducing the dataset from `52584` to `50387` rows.\n",
        "*   Missing numerical values in features like 'PM_Dongsi', 'PM_Dongsihuan', 'PM_Nongzhanguan', 'DEWP', 'HUMI', 'PRES', 'TEMP', 'Iws', 'precipitation', and 'Iprec' were successfully imputed with their respective medians.\n",
        "*   The categorical feature 'cbwd' had its missing values imputed with its mode, 'SE'.\n",
        "*   Anomalous values of 999,990.0 in 'precipitation' and 'Iprec' were replaced with 0.0. After this correction, the maximum 'precipitation' value was 69.2, and the maximum 'Iprec' value was 223.0.\n",
        "*   The 'cbwd' feature was one-hot encoded using `drop_first=True`, resulting in the addition of new columns (`cbwd_NW`, `cbwd_SE`, `cbwd_cv`) and changing the feature set `X`'s shape from (50387, 16) to (50387, 18).\n",
        "*   The dataset is now entirely free of missing values and corrected for the specified anomalous values, with all features in a numerical format suitable for machine learning.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The extensive data cleaning and preprocessing steps have ensured that the dataset is robust and ready for training a classification model. The choice to drop rows with missing target values and impute feature values helps maintain data integrity for model training.\n",
        "*   The next step involves scaling the numerical features to ensure that no single feature dominates the model training due to its magnitude, followed by splitting the data into training and testing sets before model selection and training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca616b74"
      },
      "source": [
        "## Convert Target to Categorical\n",
        "\n",
        "Build and evaluate a Gradient Boosting Classifier model to predict air quality categories based on the preprocessed features from the \"BeijingPM20100101_20151231.csv\" dataset. This involves converting the continuous 'PM_US Post' variable into categorical classes, scaling numerical features, splitting the data into training and test sets, implementing the model with cross-validation, and evaluating its performance using appropriate classification metrics.\n",
        "Transform the continuous 'PM_US Post' target variable into discrete categorical classes (e.g., 'Good', 'Moderate', 'Unhealthy') based on predefined PM2.5 thresholds. This is a critical step to enable a classification task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2f1920d"
      },
      "source": [
        "To convert the continuous 'PM_US Post' variable into categorical classes, I will first define the PM2.5 thresholds and corresponding labels, then create a function to classify each PM2.5 value, and finally apply this function to the target Series `y` to create the new categorical target `y_categorical`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c31a314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7f87f3-d687-40ef-dab0-f778291bdbf2"
      },
      "source": [
        "def pm25_to_category(pm25):\n",
        "    if 0.0 <= pm25 <= 12.0:\n",
        "        return 'Good'\n",
        "    elif 12.1 <= pm25 <= 35.4:\n",
        "        return 'Moderate'\n",
        "    elif 35.5 <= pm25 <= 55.4:\n",
        "        return 'Unhealthy for Sensitive Groups'\n",
        "    elif 55.5 <= pm25 <= 150.4:\n",
        "        return 'Unhealthy'\n",
        "    elif 150.5 <= pm25 <= 250.4:\n",
        "        return 'Very Unhealthy'\n",
        "    elif pm25 > 250.4:\n",
        "        return 'Hazardous'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "y_categorical = y.apply(pm25_to_category)\n",
        "\n",
        "print(\"First 5 rows of y_categorical:\")\n",
        "print(y_categorical.head())\n",
        "print(\"\\nValue counts of y_categorical:\")\n",
        "print(y_categorical.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of y_categorical:\n",
            "23         Unhealthy\n",
            "24         Unhealthy\n",
            "25    Very Unhealthy\n",
            "26    Very Unhealthy\n",
            "27         Unhealthy\n",
            "Name: PM_US Post, dtype: object\n",
            "\n",
            "Value counts of y_categorical:\n",
            "PM_US Post\n",
            "Unhealthy                         18560\n",
            "Moderate                          10784\n",
            "Very Unhealthy                     6811\n",
            "Unhealthy for Sensitive Groups     6150\n",
            "Good                               4546\n",
            "Hazardous                          3536\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5383d8ed"
      },
      "source": [
        "## Scale Numerical Features\n",
        "Apply a standard scaler to normalize all numerical features in the dataset (excluding the one-hot encoded 'cbwd' columns). This ensures that features with larger numerical ranges do not unduly influence the Gradient Boosting model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2842ea0"
      },
      "source": [
        "\n",
        "To normalize the numerical features, I will first identify them, excluding the one-hot encoded 'cbwd' columns. Then, I will import `StandardScaler`, instantiate it, fit and transform the identified numerical features, and update the DataFrame `X`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64285981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0ea8b4-033b-4df8-d7e8-444c25dd227a"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical columns to scale, excluding one-hot encoded 'cbwd' columns\n",
        "# Also exclude 'year', 'month', 'day', 'hour', 'season' since they are temporal/categorical and do not need scaling.\n",
        "numerical_cols_to_scale = [col for col in X.select_dtypes(include=['int64', 'float64']).columns if not col.startswith('cbwd_') and col not in ['year', 'month', 'day', 'hour', 'season']]\n",
        "\n",
        "# Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the selected numerical columns\n",
        "X[numerical_cols_to_scale] = scaler.fit_transform(X[numerical_cols_to_scale])\n",
        "\n",
        "print(\"First 5 rows of X after numerical feature scaling:\")\n",
        "print(X.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of X after numerical feature scaling:\n",
            "    year  month  day  hour  season  PM_Dongsi  PM_Dongsihuan  PM_Nongzhanguan  \\\n",
            "23  2010      1    1    23       4  -0.198081      -0.173303        -0.206696   \n",
            "24  2010      1    2     0       4  -0.198081      -0.173303        -0.206696   \n",
            "25  2010      1    2     1       4  -0.198081      -0.173303        -0.206696   \n",
            "26  2010      1    2     2       4  -0.198081      -0.173303        -0.206696   \n",
            "27  2010      1    2     3       4  -0.198081      -0.173303        -0.206696   \n",
            "\n",
            "        DEWP      HUMI      PRES      TEMP       Iws  precipitation    Iprec  \\\n",
            "23 -1.338508 -0.524266  0.344392 -1.452731 -0.456249        -0.0755 -0.08331   \n",
            "24 -1.268187 -0.640056  0.344392 -1.369963 -0.437803        -0.0755 -0.08331   \n",
            "25 -1.197866 -0.485669  0.344392 -1.369963 -0.419563        -0.0755 -0.08331   \n",
            "26 -0.916581  0.344159  0.441603 -1.452731 -0.401322        -0.0755 -0.08331   \n",
            "27 -0.635296  1.173987  0.538814 -1.452731 -0.364635        -0.0755 -0.08331   \n",
            "\n",
            "    cbwd_NW  cbwd_SE  cbwd_cv  \n",
            "23    False    False     True  \n",
            "24    False     True    False  \n",
            "25    False     True    False  \n",
            "26    False     True    False  \n",
            "27    False     True    False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fdc0ba6"
      },
      "source": [
        "To prepare the data for model training and evaluation, I will split the features (X) and the categorical target (y_categorical) into training and testing sets using `train_test_split`. This will allow for proper assessment of the model's performance on unseen data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a66757e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2d5c36-0c7a-493a-fd0f-959d544d5acd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (40309, 18)\n",
            "Shape of X_test: (10078, 18)\n",
            "Shape of y_train: (40309,)\n",
            "Shape of y_test: (10078,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1f5d5b"
      },
      "source": [
        "## Implement Gradient Boosting Model with Cross-Validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f631e8c2"
      },
      "source": [
        "Initialize a Gradient Boosting Classifier. Then, use k-fold cross-validation on the training data to train and evaluate the model.\n",
        "To implement the Gradient Boosting Classifier with k-fold cross-validation, I will import the necessary modules, initialize the classifier and `StratifiedKFold`, then use `cross_val_score` to evaluate the model on the training data, and finally print the mean accuracy and standard deviation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69fd5ceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835c804c-160c-4fdd-a32e-b0941122b677"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Gradient Boosting Classifier\n",
        "gbc = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Instantiate StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(gbc, X_train, y_train, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Cross-validation Accuracy: {np.mean(cv_scores):.4f} +/- {np.std(cv_scores):.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation Accuracy: 0.6777 +/- 0.0022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f645c4aa"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "Evaluate the final Gradient Boosting model on the held-out test set. Calculate and present key classification metrics such as accuracy, precision, recall, F1-score, and a confusion matrix to assess the model's effectiveness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b577f744"
      },
      "source": [
        "To evaluate the Gradient Boosting model, I will first fit the model to the training data, then make predictions on the test set, and finally calculate and print various classification metrics including accuracy, precision, recall, F1-score, and the confusion matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26b4fc87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6072c4-bffc-4ede-eb0b-3722b6676d9a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Fit the Gradient Boosting Classifier model\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gbc.predict(X_test)\n",
        "\n",
        "# Calculate and print classification metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision (weighted): {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall (weighted): {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1-score (weighted): {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "\n",
        "# Generate and print the confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6882\n",
            "Precision (weighted): 0.6971\n",
            "Recall (weighted): 0.6882\n",
            "F1-score (weighted): 0.6746\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 488    0  346   69    2    2]\n",
            " [   0  457    4  160    0   68]\n",
            " [ 148    2 1578  384  101    5]\n",
            " [   1   40  119 3371  117  124]\n",
            " [   1    0  215  497  449   10]\n",
            " [   0   77   18  632    0  593]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb7806c7",
        "outputId": "3b85b50d-d5bf-4720-984b-21efaf156871"
      },
      "source": [
        "import time\n",
        "\n",
        "# Measure training time\n",
        "start_train_time = time.time()\n",
        "gbc.fit(X_train, y_train)\n",
        "end_train_time = time.time()\n",
        "train_time = end_train_time - start_train_time\n",
        "\n",
        "print(f\"Model training time: {train_time:.4f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training time: 49.6301 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d206fdb7",
        "outputId": "3711ac89-06d5-44d3-e008-e325c4d3dda1"
      },
      "source": [
        "# Measure prediction time\n",
        "start_predict_time = time.time()\n",
        "y_pred = gbc.predict(X_test)\n",
        "end_predict_time = time.time()\n",
        "predict_time = end_predict_time - start_predict_time\n",
        "\n",
        "print(f\"Model prediction time: {predict_time:.4f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prediction time: 0.1199 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffb7cea"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The continuous 'PM\\_US Post' variable was successfully transformed into six discrete air quality categories: 'Good', 'Moderate', 'Unhealthy for Sensitive Groups', 'Unhealthy', 'Very Unhealthy', and 'Hazardous'. 'Unhealthy' was identified as the most frequent category.\n",
        "*   Numerical features such as `PM_Dongsi`, `DEWP`, `HUMI`, `PRES`, `TEMP`, `Iws`, `precipitation`, and `Iprec` were successfully scaled using `StandardScaler`.\n",
        "*   The dataset was split into training and testing sets with an 80/20 ratio, resulting in 40,309 samples for training and 10,078 samples for testing.\n",
        "*   A Gradient Boosting Classifier, evaluated using 5-fold stratified cross-validation on the training data, achieved a mean accuracy of 0.6777 with a standard deviation of 0.0022.\n",
        "*   On the held-out test set, the final Gradient Boosting model demonstrated an accuracy of 0.6882, a weighted precision of 0.6971, a weighted recall of 0.6882, and a weighted F1-score of 0.6746.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The consistency between the cross-validation accuracy (0.6777) and the test set accuracy (0.6882) suggests that the model generalizes reasonably well to unseen data.\n",
        "*   Analyzing the confusion matrix in detail can pinpoint specific misclassification patterns, which could guide further model refinement, such as optimizing class-specific weights or exploring advanced techniques to improve performance on under-represented or highly confused categories.\n"
      ]
    }
  ]
}